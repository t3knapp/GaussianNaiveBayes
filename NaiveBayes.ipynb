{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is an overview of the Naive Bayes Classifier.\n",
        "\n",
        "Broadly, the goal of Naive Bayes is to assign a finite number of class labels to a set of problem instances. These instances are represented as vectors of features. There are many different flavors of this algorithm, but what they have in common is that they assume that the value of a feature is independent of any other feature, hence the \"naive\" title.\n",
        "\n",
        "Naive Bayes is a conditional probability model, so to start we will review some basics of probability that will be needed later."
      ],
      "metadata": {
        "id": "lHdarRWtVTHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Probability\n",
        "\n",
        "Consider an experiment that can produce a number of results. We call the collection of all possible events the sample space $Ω$. \n",
        "\n",
        "Given an event in this sample space $X$, we will denote the **probability** of $A$ occuring as $p(A)$. Given a second event $Y$, we can write the **joint probability** of both events occuring as $p(X,Y)$.\n",
        "\n",
        "The **conditional probability** is the probability of some event $X$, given that $Y$ has occured and is deonoted $p(X|Y)$. \n",
        "\n",
        "We can relate the joint probability and the conditional probability by the **chain rule** $p(X,Y)=p(Y|X)p(X)$. This is also called the product rule by some.\n",
        "\n",
        "\n",
        "\n",
        "**Bayes' theorem** relates the conditional probabilities $p(Y|X)=\\frac{p(X|Y)p(Y)}{p(X)}$. Using the sum rule we can express the denominator as $p(X)=\\sum_Y p(X|Y)p(Y)$ so it can be looked at as a sort of normalization constant to endure that the sum of all conditional probabilities on the left hand side $p(Y|X)$ equals 1. \n",
        "\n",
        "In plain english if we think of $Y$ as an event we are interested in and $X$ as some event that will occur, $p(Y|X)$ is the probability of $Y$ occuring now that $X$ has occured and $p(Y)$ is the probability prior to $X$ occuring. The probability $p(X|Y)$ is called the likelihood, so the relation between the conditional probabilities can be expressed as:\n",
        "\n",
        "$$\\text{posterior } \\propto \\text{ likelihood} \\times \\text{prior}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "byLzaBj2WQym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "\n",
        "We mentioned earlier that Naive Bayes is a conditional probability model meaning that given a problem instance $\\vec{x} = (x_1,...,x_n)$ with $n$ features it assigns a conditional probability $p(C_k|x_1,...x_n)$ for each of the $K$ possible classes $C_k$.\n",
        "\n",
        "From Bayes' theorem we have $p(C_k|\\vec{x})=\\frac{p(\\vec{x}|C_k)p(C_k)}{p(\\vec{x})}$. \n",
        "\n",
        "The denominator is a constant that we can write as a normalization factor with the sum rule $p(\\vec{x})=\\sum_k p(C_k)p(\\vec{x}|C_k)$.\n",
        "\n",
        "Since $\\vec{x}$ is given, we know the denominator and are only interested in computing the numerator of this equation.\n",
        "\n",
        "Notice that by the chain rule we can write the numerator as a joint probability $p(\\vec{x}|C_k)p(C_k) =p(C_k,\\vec{x}) =p(C_k,x_1,...,x_n)$. By repeatedly applying the chain rule again we can write this as:\n",
        "\n",
        "$$\\begin{align}\n",
        "p(C_k,x_1,...,x_n)&=p(x_1,...,x_n,C_k)\\\\\n",
        "&=p(x_1|x_2...,x_n,C_k)p(x_2...,x_n,C_k)\\\\\n",
        "&=p(x_1|x_2...,x_n,C_k)p(x_2|x_3...,x_n,C_k)p(x_3...,x_n,C_k)\\\\\n",
        "&=...\\\\\n",
        "&=p(x_1|x_2...,x_n,C_k)p(x_2|x_3...,x_n,C_k)...p(x_{n-1}|x_n,C_k)p(x_n|C_k)p(C_k)\\\\\n",
        "\\end{align}$$\n",
        "\n",
        "Now using our \"naive\" assumption that all of these conditional probabilities are mutually independant on the category $C_k$ we get $$p(x_i|x_{i-1},...,x_n,C_k)=p(x_i|C_k), \\forall i$$\n",
        "\n",
        "Therefore our joint model can be expressed as\n",
        "$$\\begin{align}\n",
        "p(C_k|x_1,...,x_n) &= \\frac{1}{Z}p(C_k,x_1,...,x_n)\\\\\n",
        "&= \\frac{1}{Z}p(C_k)p(x_1|C_k)p(x_2|C_k)...p(x_n|C_k)\\\\\n",
        "&= \\frac{1}{Z}p(C_k) \\Pi_{i=1}^{n}p(x_i|C_k) \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $Z=p(x)=\\sum_k p(C_k)p(\\vec{x}|C_k)$ from before.\n",
        "\n",
        "\n",
        "### What do we do with this?\n",
        "\n",
        "To turn this calculation of a conditional probability into a classifier all we have to do is combine it with a decision rule. The easiest rule to implement is to choose the hypothesis that is most probable to minimize the probability of misclassification. This called a Bayes classifier and can be written as:\n",
        "\n",
        "$$\\hat{y} = \\text{argmax}_{k\\in\\{1,...K\\}} p(C_k)\\Pi_{i=1}^n p(x_i|C_k)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "JypKsze2ikGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "We now need a way to use our formulation from above to tell us about the parameters for a feature's distribution. \n",
        "\n",
        "The first thing we need to do is have a way to calculate the classes prior $p(C_k)$. We can either assume that the classes are all equally likely or to do even better we can calculate an estimate of the class probability from the training set: $p(C_k)=\\frac{\\text{number of samples in the class}}{\\text{total number of samples}}$.\n",
        "\n",
        "To use Naive Bayes on a real world problem we need to make an assumption about the probability distribution of our data. For discrete features like document classification we can use a multinomial or Bernoulli distribution. For continuous features a popular assumption is a normal distribution. We will use these distributions when calculating the likelihoods $p(x|C_k)$ to use in our Gaussian Naive Bayes.\n"
      ],
      "metadata": {
        "id": "bQ8XkeeYuVNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Naive Bayes\n",
        "\n",
        "When our data is continuous we might assume that the values associated with each class are distributed according to a normal distribution. \n",
        "\n",
        "If we do this, we can take the training data and sort it by class and then calculate the mean $μ_k$ and variance $σ^2_k$ of the data for each class $C_k$. Then given an observation $v$, we can calculate the probability density of a civen class by plugging these values into a normal distribution:\n",
        "$$p(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi σ^2_k}} e^{{-\\frac{(v-\\mu_k)^2}{2σ^2_k} }}$$  \n",
        "\n",
        "Then use this with our priors to calculate the conditional distributions for each class and choose the class with the largest probability.\n",
        "\n",
        "Below we will implement a simple example of Gaussian Naive Bayes.\n",
        "\n",
        "We will consider the problem of classifying by gender given the continuous data of height, weight and foot size. We will use a small dataset shown below to illustrate the implementation of Gaussian Naive Bayes.\n"
      ],
      "metadata": {
        "id": "T8TibfD5Xch1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Our training dataset\n",
        "import pandas as pd\n",
        "\n",
        "train_ = pd.read_csv('/content/drive/MyDrive/NaiveBayes/Train.csv')\n",
        "print(train_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qOlB3clbr-T",
        "outputId": "85b09b4f-4a45-4c7d-fc4a-4945cfc2b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Person  Height  Weight  Foot Size\n",
            "0    male    6.00     190         12\n",
            "1    male    5.90     175         11\n",
            "2    male    5.61     165         10\n",
            "3    male    5.74     180         10\n",
            "4  female    5.10     100          6\n",
            "5  female    5.00     120          5\n",
            "6  female    5.50     131          8\n",
            "7  female    5.23     124          9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gaussian Naive Bayes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "#Creates the numerator of Naive Bayes\n",
        "def NBayes(likelihood,prior):\n",
        "  out=prior\n",
        "  for i in range(0, len(likelihood)):\n",
        "    out=out*likelihood[i]\n",
        "  return out\n",
        "\n",
        "#Calculates the likelihoods\n",
        "def Gaussian(x,mean,var):\n",
        "  #Calculates the Gaussian probability distribution\n",
        "  return (1/math.sqrt(2*math.pi*var))*math.exp(-(x-mean)*(x-mean)/(2*var))\n",
        "\n",
        "#Our test values\n",
        "test_ = pd.read_csv('/content/drive/MyDrive/NaiveBayes/Test.csv')\n",
        "x=['female',5.1,120,7]\n",
        "\n",
        "train_ = pd.read_csv('/content/drive/MyDrive/NaiveBayes/Train.csv')\n",
        "\n",
        "#Parameters for training dataframe\n",
        "n_cols=len(train_.columns)\n",
        "n_male=len(train_.loc[train_.Person == \"male\"])\n",
        "n_female=len(train_.loc[train_.Person == \"female\"])\n",
        "\n",
        "\n",
        "#Assume prior distribution of 50% males 50% Females\n",
        "prior_male = 0.5\n",
        "prior_female = 0.5\n",
        "\n",
        "#Separate males and females in dataframe\n",
        "males = train_.loc[train_.Person == \"male\"]\n",
        "females = train_.loc[train_.Person == \"female\"]\n",
        "\n",
        "#Calculating the likelihoods for each attribute\n",
        "likelihood_male=[]\n",
        "likelihood_female=[]\n",
        "\n",
        "for i in range(1,n_cols):\n",
        "  likelihood_male.append(Gaussian(x[i], males.iloc[:,i].mean(), males.iloc[:,i].var()))\n",
        "  likelihood_female.append(Gaussian(x[i], females.iloc[:,i].mean(), females.iloc[:,i].var()))\n",
        "\n",
        "\n",
        "#Using NBayes\n",
        "prob_male = NBayes(likelihood_male, prior_male)\n",
        "prob_female = NBayes(likelihood_female, prior_female)\n",
        "\n",
        "if prob_male >= prob_female:\n",
        "  print(\"Prediction is Male\")\n",
        "else:\n",
        "  print(\"Prediction is Female\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vOzbuVHK3UR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61bfc7f-4097-4436-935c-57835c4d0f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction is Female\n"
          ]
        }
      ]
    }
  ]
}